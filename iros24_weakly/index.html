<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LST-Net: An Efficient Transformer for Weakly Supervised 3D Scene Segmentation">
  <meta property="og:title" content="LST-Net: Weakly Scene Segmentation Using Efficient Transformer" />
  <meta property="og:description" content="A low-rank and sparse Transformer architecture for large-scale 3D point cloud segmentation with only 1‰ supervision." />
  <meta property="og:url" content="https://yourusername.github.io/LST-Net" />
  <meta property="og:image" content="static/images/lstnet_banner.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />

  <meta name="twitter:title" content="LST-Net for Weakly Supervised 3D Segmentation">
  <meta name="twitter:description" content="Efficient Transformer architecture for sparse supervision in point cloud segmentation.">
  <meta name="twitter:image" content="static/images/lstnet_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="point cloud, segmentation, transformer, 3D vision, weak supervision, LST-Net">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>LST-Net</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container has-text-centered">
        <h1 class="title is-1">Weakly Scene Segmentation Using Efficient Transformer</h1>

        <div class="is-size-5 publication-authors">
          <span class="author-block">
            Hao Huang, Shuaihang Yuan, CongCong Wen, Yu Hao, Yi Fang
          </span><br>
          <span class="author-block">
            Embodied AI and Robotics (AIR) Lab, New York University Abu Dhabi
          </span><br>
          <small><sup>*</sup>IROS 2024</small><br>
        </div>

        <div class="buttons is-centered" style="margin-top: 1rem;">
          <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10802479" class="button is-dark is-rounded">
            <span class="icon"><i class="fas fa-file-pdf"></i></span>
            <span>Paper</span>
          </a>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2 has-text-centered" style="color: #6b6bff;">Overview</h2>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig1.png" alt="Overview" width="900" />
        <h4 class="subtitle has-text-centered">Schematic illustration of the proposed low-rank approximation for self-attention in LST-Net.</h4>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2 has-text-centered" style="color: #6b6bff;">Abstract</h2>
      <div class="content has-text-centered">
        <p>
         Current methods for large-scale point cloud scene
 semantic segmentation rely on manually annotated dense point
wise labels, which are costly, labor-intensive, and prone to
 errors. Consequently, gathering point cloud scenes with billions
 of labeled points is impractical in real-world scenarios. In this
 paper, we introduce a novel weak supervision approach to
 semantically segment large-scale indoor scenes, requiring only
 1‰ of the points to be labeled. Specifically, we develop an
 efficient point neighbor Transformer to capture the geometry of
 local point cloud patches. To address the quadratic complexity
 of self-attention computation in Transformers, particularly for
 large-scale point clouds, we propose approximating the self
attention matrix using low-rank and sparse decomposition.
 Building on the point neighbor Transformer as foundational
 blocks, we design a Low-rank Sparse Transformer Network (LST
Net) for weakly supervised large-scale point cloud scene seman
tic segmentation. Experimental results on two commonly used
 indoor point cloud scene segmentation benchmarks demon
strate that our model achieves performance comparable to those
 of both weakly supervised and fully supervised methods.
        </p>
      </div>
    </div>
  </section>

  <section class="section has-background-light">
    <div class="container is-max-desktop">
      <h2 class="title is-2 has-text-centered" style="color: #6b6bff;">Method</h2>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig3.png" alt="Method" width="900" />
        <h4 class="subtitle has-text-centered"> The architecture of LST-Net. For a labeled point, it queries features from four point neighbor Transformer blocks
 based on spatial affinity (black dashed and green solid lines). The queried features are concatenated and classified by an MLP. “DS” denotes
 FPS down-sampling.</h4>
      </div>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig2.png" alt="Method" width="600" />
        <h4 class="subtitle has-text-centered">We approximate the SA matrix in our point neighbor
 Transformer with a low-rank matrix and a sparse matrix</h4>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-2 has-text-centered" style="color: #6b6bff;">Results</h2>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig4.png" alt="Results 1" width="900" />
        <h4 class="subtitle has-text-centered">Visualization of semantic segmentation results on the S3DIS dataset.</h4>
      </div>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig5.png" alt="Results 2" width="900" />
        <h4 class="subtitle has-text-centered">Visualization of semantic segmentation results on the ScanNet dataset.</h4>
      </div>
      <div class="hero-body has-text-centered">
        <img src="static/images/iros24_weakly_fig6.png" alt="Results 2" width="600" />
        <h4 class="subtitle has-text-centered"> Mean IoU scores of different approaches on
 ScanNet are reported.</h4>
      </div>
    </div>
  </section>

  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{huang2024weakly,
          title={Weakly Scene Segmentation Using Efficient Transformer},
          author={Huang, Hao and Yuan, Shuaihang and Wen, CongCong and Hao, Yu and Fang, Yi},
          booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
          pages={9784--9790},
          year={2024},
          organization={IEEE}
        }
      </code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          Built with the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.<br>
          Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
        </p>
      </div>
    </div>
  </footer>
</body>

</html>
