<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="description" content="Embodied RobAI: Advancing Embodied AI through Vision-Language Actions for General-Purpose Robots.">
  <meta property="og:title" content="Embodied RobAI" />
  <meta property="og:description" content="Unified Vision-Language and Action Intelligence for Robotic Autonomy." />
  <meta property="og:url" content="https://yourusername.github.io/embodied-robai/" />
  <meta property="og:image" content="static/images/embodied_robot.png" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Embodied RobAI</title>
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
</head>

<body>
  <section class="hero">
    <div class="hero-body has-text-centered">
      <h1 class="title is-1">Embodied RobAI: Advancing Embodied AI through Vision-Language Actions</h1>
      <h2 class="subtitle is-4">NYU AIR Lab</h2>
      <p>New York University, NYU Abu Dhabi</p>
      <span class="author-block" style="background: linear-gradient(to right, orange, green); -webkit-background-clip: text; -webkit-text-fill-color: transparent;"><b>Ongoing Project</b></span>
    </div>
  </section>

  <section class="section">
    <div class="container content">
      <h2 class="title is-2 has-text-centered" style="color:#6b6bff;">Project Summary</h2>
      <p>
        The Embodied RobAI project is a bold initiative to push the frontier of Embodied Artificial Intelligence (Embodied AI)—where robots not only process data but live and act in the physical world through sensors, perception, and motor control. At the heart of this project is the development of Vision-Language Actions (VLA), a powerful paradigm that connects visual understanding, natural language comprehension, and robotic execution into a unified, intelligent system.
      </p>
      <p>
        Unlike traditional AI models, which operate in abstract digital environments, Embodied RobAI builds physically grounded intelligence: robots that see like humans, understand like humans, and act in the world like humans. These systems can manipulate objects, navigate dynamic environments, and follow human instructions seamlessly, achieving true interactive autonomy.
      </p>

      <h3 class="title is-3">Why Embodied AI?</h3>
      <p>
        Embodied AI represents the future of intelligent systems. It merges perception, cognition, and action within a real-world feedback loop. Through embodiment, AI agents learn by doing—developing spatial, physical, and commonsense reasoning that purely digital agents cannot achieve. The Embodied RobAI project emphasizes this principle by embedding intelligence into robotic bodies, enabling situational awareness, goal-oriented behavior, and real-time decision-making in the wild.
      </p>

      <h3 class="title is-3">Core Objectives</h3>
      <ul>
        <li><strong>Unifying VLA and Embodied AI:</strong> Harness Vision-Language Actions to tightly couple perception, language, and control within robotic platforms for lifelike understanding and reactivity.</li>
        <li><strong>Learning from Interaction:</strong> Enable embodied agents to learn directly from visual-linguistic interactions, acquiring new skills without the need for explicit programming or rigid rule-based systems.</li>
        <li><strong>Scalable Generalization:</strong> Build systems that generalize across tasks, environments, and object types using pretrained VLA models adapted through embodied experiences.</li>
        <li><strong>Grounded Instruction Following:</strong> Equip robots to understand and execute natural language commands grounded in what they currently see and where they are—supporting interactive, context-aware task execution.</li>
      </ul>

      <h3 class="title is-3">System Architecture</h3>
      <ul>
        <li><strong>Embodied Visual Perception:</strong> Real-time processing of first-person (egocentric) camera input for scene and object recognition.</li>
        <li><strong>Language-Guided Decision Making:</strong> Use of foundation language models aligned with visual context to interpret and plan.</li>
        <li><strong>Action Controller:</strong> Direct actuation of robotic limbs and locomotion systems using policy modules trained via VLA-based reinforcement learning or imitation learning.</li>
        <li><strong>Embodied Memory Module:</strong> Spatial and temporal memory supporting persistent awareness of task context and environmental changes.</li>
      </ul>

      <h3 class="title is-3">Key Applications</h3>
      <ul>
        <li>Assistive Embodied Agents: Robotic companions for blind, elderly, or mobility-impaired individuals.</li>
        <li>Home and Service Robotics: Cleaning, fetching, organizing, and real-time help based on voice commands.</li>
        <li>Interactive Learning Robots: Educational tools that demonstrate Embodied AI in action through real-world exploration and interaction.</li>
        <li>Industrial Adaptation: Flexible task-performing agents in logistics and warehouse settings.</li>
      </ul>

      <h3 class="title is-3">Long-Term Vision</h3>
      <p>
        The Embodied RobAI project envisions a future in which robots possess truly embodied intelligence—intelligence that is physical, interactive, and adaptable. By integrating Vision-Language Actions as the bridge between perception and execution, we are building the foundation for general-purpose robots that do more than think—they see, understand, and act. This is not just robotics. This is Embodied AI, brought to life.
      </p>
    </div>
  </section>

  <section class="section has-background-light">
    <div class="container">
      <h2 class="title is-2 has-text-centered" style="color:#6b6bff;">Video Showcase</h2>

      <div class="content has-text-centered">
        <h3 class="title is-4">Robotic Arm</h3>
        <iframe id="roboticArmVideo" width="800" height="450" src="https://drive.google.com/file/d/YOUR_FILE_ID_1/preview" allow="autoplay" allowfullscreen></iframe>

        <h3 class="title is-4 mt-6">Humanoid Robot</h3>
        <iframe id="humanoidVideo" width="800" height="450" src="https://drive.google.com/file/d/YOUR_FILE_ID_2/preview" allow="autoplay" allowfullscreen></iframe>

        <h3 class="title is-4 mt-6">Autonomous Car</h3>
        <iframe id="carVideo" width="800" height="450" src="https://drive.google.com/file/d/YOUR_FILE_ID_3/preview" allow="autoplay" allowfullscreen></iframe>

        <h3 class="title is-4 mt-6">Quadruped Robot</h3>
        <iframe id="quandraVideo" width="800" height="450" src="https://drive.google.com/file/d/YOUR_FILE_ID_4/preview" allow="autoplay" allowfullscreen></iframe>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
      <p>
        Built with ❤️ using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.<br>
        Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
      </p>
    </div>
  </footer>
</body>

</html>
